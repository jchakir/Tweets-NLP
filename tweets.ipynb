{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QefDXqpSEqbu"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4AZwAuEEqbx",
    "outputId": "069a9163-e045-469f-bb15-8182bf3e14b4"
   },
   "outputs": [],
   "source": [
    "# ! pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QZPUWxMCEqbz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from typing import Literal, Any\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas._typing import ArrayLike\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords, words as nltk_words\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "useh2uPOEqb1"
   },
   "source": [
    "### Download NLTK Ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YI1LOJNsEqb2",
    "outputId": "5d84e131-8360-4616-f956-456ee61e542d"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExRmGTUjEqb3"
   },
   "source": [
    "### Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dsmN9Kk8Eqb4"
   },
   "outputs": [],
   "source": [
    "def __tweet__clean(sent: str) -> str:\n",
    "  sent = re.sub(r'http\\S+', '', sent)\n",
    "  sent = re.sub(r'[^A-Za-z\\s]', ' ', sent)\n",
    "  sent = re.sub(r'^\\s+|\\s+$', '', sent, flags=re.MULTILINE)\n",
    "  sent = re.sub(r'\\s{2,}', ' ', sent)\n",
    "  return sent.lower()\n",
    "\n",
    "def __tweet__preprocess(\n",
    "    tweets_df: pd.Series, method: Literal['stem', 'lemmatize']|None=None, misspelling=False, stopword=False\n",
    "    ) -> pd.Series:\n",
    "  stop_words = set(nltk_stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  stemmer    = PorterStemmer()\n",
    "  speller    = SpellChecker(distance=1)\n",
    "  speller.word_frequency.load_words(nltk_words.words())\n",
    "  def __preprocess_helper(tweet: str) -> str:\n",
    "    # words = word_tokenize(tweet)\n",
    "    words = tweet.split()\n",
    "    clean_words: list[str] = []\n",
    "    for word in words:\n",
    "      if misspelling:\n",
    "        correct_word = speller.correction(word)\n",
    "        word = word if correct_word is None else correct_word\n",
    "      if stopword and word in stop_words:\n",
    "        continue\n",
    "      if method == 'stem':\n",
    "        word = stemmer.stem(word)\n",
    "      if method == 'lemmatize':\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "      clean_words.append(word)\n",
    "    return ' '.join(clean_words)\n",
    "  # begin preprocess\n",
    "  processed_tweet = tweets_df.apply(__tweet__clean)\n",
    "  if method:\n",
    "    processed_tweet = processed_tweet.apply(__preprocess_helper)\n",
    "  return processed_tweet\n",
    "\n",
    "def __tweet__vectorizer(\n",
    "  tweets: pd.Series, *, method: Literal['count', 'tfidf', 'word2vec'], binary=False, ngram: Literal['11', '12', '22']='11'\n",
    "  )-> np.ndarray:\n",
    "  ngram_range = {'11': (1, 1), '12': (1, 2), '22': (2, 2)}.get(ngram, (1, 1))\n",
    "  if method == 'tfidf':\n",
    "    bow = TfidfVectorizer(ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  elif method == 'count':\n",
    "    bow = CountVectorizer(binary=binary, ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  # block for Word2Vec\n",
    "  bow_w2v: np.ndarray = np.empty((0, 133+7), np.float32)\n",
    "  sentences           = [ line.split() for line in tweets ]\n",
    "  w2v                 = Word2Vec(sentences, vector_size=133+7, epochs=13, min_count=1)\n",
    "  for sent in sentences:\n",
    "    tweet_vec = np.zeros((1, 133+7))\n",
    "    for word in sent:\n",
    "      tweet_vec += w2v.wv[word]\n",
    "    bow_w2v = np.append(bow_w2v, tweet_vec, axis=0)\n",
    "  return bow_w2v\n",
    "\n",
    "def __load_tweets_and_labels_into_dataframe() -> pd.DataFrame:\n",
    "  # load dataset\n",
    "  neg, neu, pos = './data/negative.csv', './data/neutral.csv', './data/positive.csv'\n",
    "  with open(neg, 'r') as neg_f, open(neu, 'r') as neu_f, open(pos, 'r') as pos_f:\n",
    "    negative, neutral, positive = neg_f.read(), neu_f.read(), pos_f.read()\n",
    "  # sent tokenizing\n",
    "  sentence_pattern = r',([A-Z])'\n",
    "  replacement_pattern = r'\\n\\1'\n",
    "  negative = re.sub(sentence_pattern, replacement_pattern, negative).splitlines()\n",
    "  neutral  = re.sub(sentence_pattern, replacement_pattern, neutral ).splitlines()\n",
    "  positive = re.sub(sentence_pattern, replacement_pattern, positive).splitlines()\n",
    "  # create DataFrame for each label\n",
    "  negative = pd.DataFrame({'tweet': negative, 'label': 0})\n",
    "  neutral  = pd.DataFrame({'tweet': neutral, 'label': 1})\n",
    "  positive = pd.DataFrame({'tweet': positive, 'label': 2})\n",
    "  # concat DataFrame\n",
    "  tweets_df = pd.concat([negative, neutral, positive])\n",
    "  # remove empty tweets\n",
    "  tweets_df = tweets_df[tweets_df['tweet'].str.strip() != '']\n",
    "  tweets_df = tweets_df.drop_duplicates(subset=['tweet'], ignore_index=True)\n",
    "  tweets_df = tweets_df.sample(frac=1, ignore_index=True) # here shuffle tweets\n",
    "  return tweets_df\n",
    "\n",
    "def tweets_load() -> tuple[ArrayLike, ArrayLike, dict[str, np.ndarray]]:\n",
    "  \"\"\"\n",
    "  Return:\n",
    "  -------\n",
    "  tweets, y_labels, bows\n",
    "  \"\"\"\n",
    "  # processing and vectorizing with its params\n",
    "  processing  = ['just_tokenization', 'stemming', 'lemmatization', 'stemming+misspelling', 'lemmatization+misspelling', 'lemmatization+stopwords']\n",
    "  processing_params  = [\n",
    "    {'method': None}, {'method': 'stem'}, {'method': 'lemmatize'}, {'method': 'stem', 'misspelling': True},\n",
    "    {'method': 'lemmatize', 'misspelling': True}, {'method': 'lemmatize', 'stopword': True}\n",
    "    ]\n",
    "  vectorizing = ['binary', 'word_counts', 'tfidf', 'word2vec']\n",
    "  vectorizing_params = [{'method': 'count', 'binary': True}, {'method': 'count'}, {'method': 'tfidf'}, {'method': 'word2vec'}]\n",
    "  # other varibles\n",
    "  bows: dict[str, np.ndarray] = {}\n",
    "  # load datasets\n",
    "  tweets_df  = __load_tweets_and_labels_into_dataframe()\n",
    "  # fit each processing method a vectorizer\n",
    "  __i, __len = 0, len(processing) * len(vectorizing)\n",
    "  for proc, proc_params in zip(processing, processing_params):\n",
    "    proc_tweets = __tweet__preprocess(tweets_df['tweet'], **proc_params)\n",
    "    for vect, vect_params in zip(vectorizing, vectorizing_params):\n",
    "      bows[f'{proc:27} and   {vect:13} vectorizing'] = __tweet__vectorizer(proc_tweets, **vect_params)\n",
    "      __i += 1\n",
    "      print(f'{__i:2}/{__len}:   {proc:27} and   {vect:13} vectorizing', end='\\r')\n",
    "  # return pure tweets, labels and bows\n",
    "  tweets, y = tweets_df['tweet'].values, tweets_df['label'].values\n",
    "  return tweets, y, bows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XdkXdYEEqb6"
   },
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3p825hxREqb6"
   },
   "outputs": [],
   "source": [
    "# def get_top_10_most_similar_tweets(bow: np.ndarray, tweets: pd.DataFrame, tweet_idx: int) -> np.ndarray:\n",
    "#   bow_sum       = np.sqrt(np.sum(np.square(bow), axis=1))\n",
    "#   bow_dot_tweet = np.dot(bow, bow[tweet_idx])\n",
    "#   similarity    = np.divide(bow_dot_tweet, bow_sum * bow_sum[tweet_idx] + 1e-7)\n",
    "#   top_10_df     = pd.DataFrame({'0': similarity}).sort_values(by='0', ascending=False).drop(index=tweet_idx)[:10]\n",
    "#   top_10_index  = top_10_df[top_10_df['0'] > 0].index\n",
    "#   top_10_tweets = tweets['tweet'].loc[top_10_index].values\n",
    "#   return top_10_tweets # type: ignore\n",
    "\n",
    "\n",
    "def __top_similar_pairs(bow: np.ndarray) -> dict[tuple[int, int], float]:\n",
    "  similarity = pd.DataFrame(cosine_similarity(bow))\n",
    "  similar_pairs: dict[tuple[int, int], float] = {}\n",
    "  for tweet_idx in similarity.columns.values:\n",
    "    tweet_similiraty = similarity[tweet_idx].sort_values(ascending=False)\n",
    "    top_value, top_idx = tweet_similiraty[1], tweet_similiraty.index[1]\n",
    "    similar_pairs[(tweet_idx, top_idx)] = top_value\n",
    "  sorted_similar_pairs_by_value = {key: similar_pairs[key] for key in sorted(similar_pairs, key=lambda itm: similar_pairs[itm], reverse=True)}\n",
    "  return sorted_similar_pairs_by_value\n",
    "\n",
    "def __print_top_similar_tweets_pair(tweets: ArrayLike, bows: dict[str, np.ndarray]) -> dict[str, list]:\n",
    "  for bow_name in bows:\n",
    "    X = bows[bow_name]\n",
    "    top_similar_pairs = __top_similar_pairs(X)\n",
    "    print(\"top similar pairs of  '\", bow_name, \"':\", sep='')\n",
    "    __i = 1\n",
    "    for pair in top_similar_pairs:\n",
    "      first, second = pair\n",
    "      print(' ' * 15, '-' * 150)\n",
    "      print(' ' * 5, f'{1.0:3f}: {tweets[first]}')\n",
    "      print(' ' * 5, f'{top_similar_pairs[pair]:3f}: {tweets[second]}')\n",
    "      if __i == 1: break # modify 1 as your need to show more similar tweets pair\n",
    "      __i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OirxTHbEqb7"
   },
   "source": [
    "### Just Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liYBhEcjEqb9",
    "outputId": "60ec7e3e-48a8-4628-9076-1a4b6466a3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24:   lemmatization+stopwords     and   word2vec      vectorizing"
     ]
    }
   ],
   "source": [
    "tweets, y, bows = tweets_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Hf94w3mEqb-"
   },
   "source": [
    "### Global Varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iBF16GHDEqb-"
   },
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "train_size: int = int(y.shape[0] * train_frac)\n",
    "\n",
    "y_train, y_test = y[: train_size], y[train_size: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNdAQJY7Eqb-"
   },
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N26wpCrTEqb-"
   },
   "outputs": [],
   "source": [
    "__print_top_similar_tweets_pair(tweets, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1nKGr3sEqb-",
    "outputId": "10630641-c066-480f-f4b1-31299cafa7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96:   lemmatization+stopwords     and   word2vec      vectorizing using   GaussianNB                model: 0.6056338028169014"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "bow_to_model: dict[str, Any] = {}\n",
    "\n",
    "__i, __len = 0, len(bows) * 4 # ( * number ) the number changes respectively with how many model you use in models: list\n",
    "for bow_name in bows:\n",
    "  X = bows[bow_name]\n",
    "  if isinstance(X, csr_matrix):\n",
    "    X = X.toarray()\n",
    "  X_train, X_test = X[: train_size], X[train_size: ]\n",
    "  models: list = [\n",
    "      LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=37),\n",
    "      DecisionTreeClassifier(),\n",
    "      BernoulliNB(),\n",
    "      GaussianNB(),\n",
    "    ]\n",
    "  for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    model_name = model.__class__.__name__\n",
    "    bow_to_model[f'{bow_name} using   {model_name:25} model'] = accuracy\n",
    "    __i += 1\n",
    "    print(f'{__i:2}/{__len}:   {bow_name} using   {model_name:25} model: {accuracy}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhm202TaEqb-",
    "outputId": "bacad56b-caa4-4d49-f280-90c5c61b7c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming                    and   binary        vectorizing using   LogisticRegression        model:   0.9272300\n",
      "stemming+misspelling        and   binary        vectorizing using   LogisticRegression        model:   0.9272300\n",
      "stemming                    and   word_counts   vectorizing using   LogisticRegression        model:   0.9225352\n",
      "just_tokenization           and   word_counts   vectorizing using   LogisticRegression        model:   0.9154930\n",
      "just_tokenization           and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.9154930\n",
      "stemming                    and   binary        vectorizing using   BernoulliNB               model:   0.9154930\n",
      "stemming                    and   word_counts   vectorizing using   BernoulliNB               model:   0.9154930\n",
      "stemming                    and   tfidf         vectorizing using   BernoulliNB               model:   0.9154930\n",
      "lemmatization               and   binary        vectorizing using   LogisticRegression        model:   0.9154930\n",
      "stemming+misspelling        and   word_counts   vectorizing using   LogisticRegression        model:   0.9154930\n",
      "lemmatization+misspelling   and   binary        vectorizing using   LogisticRegression        model:   0.9154930\n",
      "lemmatization+stopwords     and   binary        vectorizing using   LogisticRegression        model:   0.9154930\n",
      "just_tokenization           and   binary        vectorizing using   LogisticRegression        model:   0.9131455\n",
      "stemming+misspelling        and   binary        vectorizing using   BernoulliNB               model:   0.9131455\n",
      "stemming+misspelling        and   word_counts   vectorizing using   BernoulliNB               model:   0.9131455\n",
      "stemming+misspelling        and   tfidf         vectorizing using   BernoulliNB               model:   0.9131455\n",
      "lemmatization               and   word_counts   vectorizing using   LogisticRegression        model:   0.9107981\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   LogisticRegression        model:   0.9084507\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   LogisticRegression        model:   0.9084507\n",
      "stemming                    and   tfidf         vectorizing using   LogisticRegression        model:   0.9061033\n",
      "lemmatization+misspelling   and   binary        vectorizing using   BernoulliNB               model:   0.9061033\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   BernoulliNB               model:   0.9061033\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   BernoulliNB               model:   0.9061033\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.9061033\n",
      "just_tokenization           and   binary        vectorizing using   DecisionTreeClassifier    model:   0.9037559\n",
      "just_tokenization           and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.9037559\n",
      "stemming                    and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.9037559\n",
      "lemmatization               and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.9037559\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.9037559\n",
      "lemmatization+stopwords     and   binary        vectorizing using   BernoulliNB               model:   0.9037559\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   BernoulliNB               model:   0.9037559\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   BernoulliNB               model:   0.9037559\n",
      "lemmatization               and   binary        vectorizing using   DecisionTreeClassifier    model:   0.9014085\n",
      "stemming+misspelling        and   tfidf         vectorizing using   LogisticRegression        model:   0.9014085\n",
      "just_tokenization           and   binary        vectorizing using   BernoulliNB               model:   0.8990610\n",
      "just_tokenization           and   word_counts   vectorizing using   BernoulliNB               model:   0.8990610\n",
      "just_tokenization           and   tfidf         vectorizing using   LogisticRegression        model:   0.8990610\n",
      "just_tokenization           and   tfidf         vectorizing using   BernoulliNB               model:   0.8990610\n",
      "lemmatization               and   binary        vectorizing using   BernoulliNB               model:   0.8990610\n",
      "lemmatization               and   word_counts   vectorizing using   BernoulliNB               model:   0.8990610\n",
      "lemmatization               and   tfidf         vectorizing using   BernoulliNB               model:   0.8990610\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.8990610\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   LogisticRegression        model:   0.8990610\n",
      "lemmatization+misspelling   and   binary        vectorizing using   DecisionTreeClassifier    model:   0.8967136\n",
      "stemming                    and   binary        vectorizing using   DecisionTreeClassifier    model:   0.8943662\n",
      "lemmatization               and   tfidf         vectorizing using   LogisticRegression        model:   0.8943662\n",
      "stemming+misspelling        and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.8943662\n",
      "stemming                    and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.8920188\n",
      "lemmatization               and   tfidf         vectorizing using   DecisionTreeClassifier    model:   0.8920188\n",
      "stemming+misspelling        and   binary        vectorizing using   DecisionTreeClassifier    model:   0.8896714\n",
      "stemming+misspelling        and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.8896714\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   LogisticRegression        model:   0.8849765\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   DecisionTreeClassifier    model:   0.8779343\n",
      "lemmatization+stopwords     and   binary        vectorizing using   DecisionTreeClassifier    model:   0.8708920\n",
      "stemming                    and   word2vec      vectorizing using   LogisticRegression        model:   0.8356808\n",
      "stemming+misspelling        and   word2vec      vectorizing using   LogisticRegression        model:   0.8333333\n",
      "just_tokenization           and   word2vec      vectorizing using   LogisticRegression        model:   0.8262911\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   LogisticRegression        model:   0.8239437\n",
      "lemmatization               and   word2vec      vectorizing using   LogisticRegression        model:   0.8192488\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   LogisticRegression        model:   0.8098592\n",
      "stemming                    and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.8028169\n",
      "lemmatization               and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.7910798\n",
      "stemming                    and   word_counts   vectorizing using   GaussianNB                model:   0.7887324\n",
      "just_tokenization           and   word_counts   vectorizing using   GaussianNB                model:   0.7863850\n",
      "stemming                    and   binary        vectorizing using   GaussianNB                model:   0.7840376\n",
      "lemmatization               and   word_counts   vectorizing using   GaussianNB                model:   0.7816901\n",
      "just_tokenization           and   binary        vectorizing using   GaussianNB                model:   0.7793427\n",
      "lemmatization               and   binary        vectorizing using   GaussianNB                model:   0.7793427\n",
      "stemming+misspelling        and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.7793427\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.7793427\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   GaussianNB                model:   0.7769953\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   GaussianNB                model:   0.7746479\n",
      "lemmatization+stopwords     and   binary        vectorizing using   GaussianNB                model:   0.7746479\n",
      "stemming+misspelling        and   word_counts   vectorizing using   GaussianNB                model:   0.7699531\n",
      "lemmatization+misspelling   and   binary        vectorizing using   GaussianNB                model:   0.7699531\n",
      "just_tokenization           and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.7676056\n",
      "just_tokenization           and   word2vec      vectorizing using   BernoulliNB               model:   0.7676056\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   BernoulliNB               model:   0.7676056\n",
      "stemming+misspelling        and   binary        vectorizing using   GaussianNB                model:   0.7652582\n",
      "stemming                    and   tfidf         vectorizing using   GaussianNB                model:   0.7558685\n",
      "just_tokenization           and   tfidf         vectorizing using   GaussianNB                model:   0.7511737\n",
      "stemming                    and   word2vec      vectorizing using   BernoulliNB               model:   0.7511737\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   GaussianNB                model:   0.7511737\n",
      "lemmatization               and   tfidf         vectorizing using   GaussianNB                model:   0.7488263\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   GaussianNB                model:   0.7464789\n",
      "lemmatization               and   word2vec      vectorizing using   BernoulliNB               model:   0.7417840\n",
      "stemming+misspelling        and   tfidf         vectorizing using   GaussianNB                model:   0.7323944\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   DecisionTreeClassifier    model:   0.7230047\n",
      "stemming+misspelling        and   word2vec      vectorizing using   BernoulliNB               model:   0.6924883\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   BernoulliNB               model:   0.6455399\n",
      "lemmatization               and   word2vec      vectorizing using   GaussianNB                model:   0.6079812\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   GaussianNB                model:   0.6079812\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   GaussianNB                model:   0.6056338\n",
      "stemming+misspelling        and   word2vec      vectorizing using   GaussianNB                model:   0.6032864\n",
      "just_tokenization           and   word2vec      vectorizing using   GaussianNB                model:   0.5938967\n",
      "stemming                    and   word2vec      vectorizing using   GaussianNB                model:   0.5868545\n"
     ]
    }
   ],
   "source": [
    "sorted_bow_to_model__keys = sorted(bow_to_model, key = lambda item: bow_to_model[item], reverse=True)\n",
    "\n",
    "for key in sorted_bow_to_model__keys:\n",
    "  value = bow_to_model[key]\n",
    "  print(f'{key}:   {value:.7f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAtKKgV-Eqb_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
