{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from typing import Literal, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas._typing import ArrayLike\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords as nltk_stopwords, words as nltk_words\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __tweet__clean(sent: str) -> str:\n",
    "  sent = re.sub(r'http\\S+', '', sent)\n",
    "  sent = re.sub(r'[^A-Za-z\\s]', ' ', sent)\n",
    "  sent = re.sub(r'^\\s+|\\s+$', '', sent, flags=re.MULTILINE)\n",
    "  sent = re.sub(r'\\s{2,}', ' ', sent)\n",
    "  return sent.lower()\n",
    "\n",
    "def __tweet__preprocess(\n",
    "    tweets_df: pd.Series, method: Literal['stem', 'lemmatize']|None=None, misspelling=False, stopword=False\n",
    "    ) -> pd.Series:\n",
    "  stop_words = set(nltk_stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  stemmer    = PorterStemmer()\n",
    "  speller    = SpellChecker(distance=1)\n",
    "  speller.word_frequency.load_words(nltk_words.words())\n",
    "  def __preprocess_helper(tweet: str) -> str:\n",
    "    # words = word_tokenize(tweet)\n",
    "    words = tweet.split()\n",
    "    clean_words: list[str] = []\n",
    "    for word in words:\n",
    "      if misspelling:\n",
    "        correct_word = speller.correction(word)\n",
    "        word = word if correct_word is None else correct_word\n",
    "      if stopword and word in stop_words:\n",
    "        continue\n",
    "      if method == 'stem':\n",
    "        word = stemmer.stem(word)\n",
    "      if method == 'lemmatize':\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "      clean_words.append(word)\n",
    "    return ' '.join(clean_words)\n",
    "  # begin preprocess\n",
    "  processed_tweet = tweets_df.apply(__tweet__clean)\n",
    "  if method:\n",
    "    processed_tweet = processed_tweet.apply(__preprocess_helper)\n",
    "  # processed_tweet = processed_tweet.drop_duplicates(ignore_index=True)\n",
    "  return processed_tweet\n",
    "\n",
    "def __tweet__vectorizer(\n",
    "  tweets: pd.Series, *, method: Literal['count', 'tfidf', 'word2vec'], binary=False, ngram: Literal['11', '12', '22']='11'\n",
    "  )-> np.ndarray:\n",
    "  ngram_range = {'11': (1, 1), '12': (1, 2), '22': (2, 2)}.get(ngram, (1, 1))\n",
    "  if method == 'tfidf':\n",
    "    bow = TfidfVectorizer(ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  elif method == 'count':\n",
    "    bow = CountVectorizer(binary=binary, ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  # block for Word2Vec\n",
    "  bow_w2v: np.ndarray = np.empty((0, 133+7), np.float32)\n",
    "  sentences           = [ line.split() for line in tweets ]\n",
    "  w2v                 = Word2Vec(sentences, vector_size=133+7, epochs=13, min_count=1)\n",
    "  for sent in sentences:\n",
    "    tweet_vec = np.zeros((1, 133+7))\n",
    "    for word in sent:\n",
    "      tweet_vec += w2v.wv[word]\n",
    "    bow_w2v = np.append(bow_w2v, tweet_vec, axis=0)\n",
    "  return bow_w2v\n",
    "\n",
    "def __load__twitter_data(n_tweet: int) -> pd.DataFrame:\n",
    "  twitter_data = pd.read_csv('./data/twitter_data.csv')\n",
    "  twitter_data = twitter_data.sample(n=n_tweet, ignore_index=True)\n",
    "  twitter_data = twitter_data.dropna()\n",
    "  twitter_data = twitter_data.rename(columns={'clean_text': 'tweet', 'category': 'label'})\n",
    "  twitter_data['label'] = twitter_data['label'].map({-1.0: 0, 0.0: 1, 1.0: 2})\n",
    "  return twitter_data\n",
    "\n",
    "def __load_tweets_and_labels_into_dataframe() -> pd.DataFrame:\n",
    "  # load dataset\n",
    "  neg, neu, pos = './data/negative.csv', './data/neutral.csv', './data/positive.csv'\n",
    "  with open(neg, 'r') as neg_f, open(neu, 'r') as neu_f, open(pos, 'r') as pos_f:\n",
    "    negative, neutral, positive = neg_f.read(), neu_f.read(), pos_f.read()\n",
    "  # sent tokenizing\n",
    "  sentence_pattern = r',([A-Z])'\n",
    "  replacement_pattern = r'\\n\\1'\n",
    "  negative = re.sub(sentence_pattern, replacement_pattern, negative).splitlines()\n",
    "  neutral  = re.sub(sentence_pattern, replacement_pattern, neutral ).splitlines()\n",
    "  positive = re.sub(sentence_pattern, replacement_pattern, positive).splitlines()\n",
    "  # create DataFrame for each label\n",
    "  negative = pd.DataFrame({'tweet': negative, 'label': 0})\n",
    "  neutral  = pd.DataFrame({'tweet': neutral, 'label': 1})\n",
    "  positive = pd.DataFrame({'tweet': positive, 'label': 2})\n",
    "  # concat DataFrame\n",
    "  tweets_df = pd.concat([negative, neutral, positive])\n",
    "  # remove empty tweets\n",
    "  tweets_df = tweets_df[tweets_df['tweet'].str.strip() != '']\n",
    "  tweets_df = tweets_df.drop_duplicates(subset=['tweet'], ignore_index=True)\n",
    "  tweets_df = tweets_df.sample(frac=1, ignore_index=True) # here shuffle tweets\n",
    "  return tweets_df\n",
    "\n",
    "def tweets_load() -> tuple[ArrayLike, ArrayLike, dict[str, np.ndarray]]:\n",
    "  \"\"\"\n",
    "  Return:\n",
    "  -------\n",
    "  tweets, y_labels, bows\n",
    "  \"\"\"\n",
    "  # processing and vectorizing with its params\n",
    "  processing  = ['just_tokenization', 'stemming', 'lemmatization', 'stemming+misspelling', 'lemmatization+misspelling', 'lemmatization+stopwords']\n",
    "  processing_params  = [\n",
    "    {'method': None}, {'method': 'stem'}, {'method': 'lemmatize'}, {'method': 'stem', 'misspelling': True}, \n",
    "    {'method': 'lemmatize', 'misspelling': True}, {'method': 'lemmatize', 'stopword': True}\n",
    "    ]\n",
    "  vectorizing = ['binary', 'word_counts', 'tfidf', 'word2vec']\n",
    "  vectorizing_params = [{'method': 'count', 'binary': True}, {'method': 'count'}, {'method': 'tfidf'}, {'method': 'word2vec'}]\n",
    "  # vectorizing = ['word2vec']\n",
    "  # vectorizing_params = [{'method': 'word2vec'}]\n",
    "  # other varibles\n",
    "  bows: dict[str, np.ndarray] = {}\n",
    "  # load datasets\n",
    "  our_dataset_df  = __load_tweets_and_labels_into_dataframe()\n",
    "  twitter_data_df = __load__twitter_data(1)\n",
    "  # concat datasets\n",
    "  tweets_df = pd.concat([our_dataset_df, twitter_data_df], ignore_index=True)\n",
    "  tweets_df = tweets_df.sample(frac=1, ignore_index=True)\n",
    "  # fit each processing method a vectorizer\n",
    "  __i, __len = 0, len(processing) * len(vectorizing) # -----------------------------\n",
    "  for proc, proc_params in zip(processing, processing_params):\n",
    "    proc_tweets = __tweet__preprocess(tweets_df['tweet'], **proc_params)\n",
    "    for vect, vect_params in zip(vectorizing, vectorizing_params):\n",
    "      bows[f'{proc:27} and   {vect:13} vectorizing'] = __tweet__vectorizer(proc_tweets, **vect_params)\n",
    "      __i += 1\n",
    "      print(f'{__i:2}/{__len}:   {proc:27} and   {vect:13} vectorizing', end='\\r')\n",
    "  # return pure tweets, labels and bows\n",
    "  tweets, y = tweets_df['tweet'].values, tweets_df['label'].values\n",
    "  return tweets, y, bows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_top_10_most_similar_tweets(bow: np.ndarray, tweets: pd.DataFrame, tweet_idx: int) -> np.ndarray:\n",
    "#   bow_sum       = np.sqrt(np.sum(np.square(bow), axis=1))\n",
    "#   bow_dot_tweet = np.dot(bow, bow[tweet_idx])\n",
    "#   similarity    = np.divide(bow_dot_tweet, bow_sum * bow_sum[tweet_idx] + 1e-7)\n",
    "#   top_10_df     = pd.DataFrame({'0': similarity}).sort_values(by='0', ascending=False).drop(index=tweet_idx)[:10]\n",
    "#   top_10_index  = top_10_df[top_10_df['0'] > 0].index\n",
    "#   top_10_tweets = tweets['tweet'].loc[top_10_index].values\n",
    "#   return top_10_tweets # type: ignore\n",
    "\n",
    "\n",
    "def __top_similar_pairs(bow: np.ndarray) -> dict[tuple[int, int], float]:\n",
    "  similarity = pd.DataFrame(cosine_similarity(bow))\n",
    "  similar_pairs: dict[tuple[int, int], float] = {}\n",
    "  for tweet_idx in similarity.columns.values:\n",
    "    tweet_similiraty = similarity[tweet_idx].sort_values(ascending=False)\n",
    "    top_value, top_idx = tweet_similiraty[1], tweet_similiraty.index[1]\n",
    "    similar_pairs[(tweet_idx, top_idx)] = top_value\n",
    "  sorted_similar_pairs_by_value = {key: similar_pairs[key] for key in sorted(similar_pairs, key=lambda itm: similar_pairs[itm], reverse=True)}\n",
    "  return sorted_similar_pairs_by_value\n",
    "\n",
    "def __print_top_similar_tweets_pair(tweets: ArrayLike, bows: dict[str, np.ndarray]) -> dict[str, list]:\n",
    "  for bow_name in bows:\n",
    "    X = bows[bow_name]\n",
    "    top_similar_pairs = __top_similar_pairs(X)\n",
    "    print(\"top similar pairs of  '\", bow_name, \"':\", sep='')\n",
    "    __i = 1\n",
    "    for pair in top_similar_pairs:\n",
    "      first, second = pair\n",
    "      print(' ' * 14, '-' * 99)\n",
    "      print(' ' * 5, f'{1.0:3f}: {tweets[first]}')\n",
    "      print(' ' * 5, f'{top_similar_pairs[pair]:3f}: {tweets[second]}')\n",
    "      if __i == 3: break\n",
    "      __i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, y, bows = tweets_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "train_size: int = math.ceil(y.shape[0] * train_frac)\n",
    "\n",
    "y_train, y_test = y[: train_size], y[train_size: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__print_top_similar_tweets_pair(tweets, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_to_model: dict[str, Any] = {}\n",
    "\n",
    "__i, __len = 0, len(bows) * 1\n",
    "for bow_name in bows:\n",
    "  X = bows[bow_name]\n",
    "  X_train, X_test = X[: train_size], X[train_size: ]\n",
    "  models: list = [\n",
    "    # LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1337),\n",
    "    # MultinomialNB(), SVC(), KNeighborsClassifier(n_neighbors=3), DecisionTreeClassifier()\n",
    "    LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1337),\n",
    "    ]\n",
    "  for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    model_name = model.__class__.__name__\n",
    "    bow_to_model[f'{bow_name} using   {model_name:25} model'] = accuracy\n",
    "    __i += 1\n",
    "    print(f'{__i:2}/{__len}:   {bow_name} using   {model_name:25} model: {accuracy}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bow_to_model__keys = sorted(bow_to_model, key = lambda item: bow_to_model[item], reverse=True)\n",
    "\n",
    "for key in sorted_bow_to_model__keys:\n",
    "  value = bow_to_model[key]\n",
    "  print(f'{key}:   {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
