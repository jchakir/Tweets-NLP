{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QefDXqpSEqbu"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "H4AZwAuEEqbx"
   },
   "outputs": [],
   "source": [
    "# ! pip install pyspellchecker fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "QZPUWxMCEqbz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from typing import Literal, Any\n",
    "import warnings\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas._typing import ArrayLike\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords, words as nltk_words\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import fasttext\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "useh2uPOEqb1"
   },
   "source": [
    "### Download NLTK Ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "YI1LOJNsEqb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExRmGTUjEqb3"
   },
   "source": [
    "### Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "dsmN9Kk8Eqb4"
   },
   "outputs": [],
   "source": [
    "def __train_fasttext__word2vec_model(sentences: pd.Series) -> object:\n",
    "  with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_file:\n",
    "    for sentence in sentences:\n",
    "      temp_file.write(f\"{sentence}\\n\")\n",
    "      temp_file_path = temp_file.name\n",
    "\n",
    "  model = fasttext.train_unsupervised(temp_file_path, model='skipgram', dim=256, epoch=15, verbose=0)\n",
    "  os.unlink(temp_file_path)\n",
    "  return model\n",
    "\n",
    "\n",
    "def __tweet__clean(sent: str) -> str:\n",
    "  sent = re.sub(r'http\\S+', '', sent)\n",
    "  sent = re.sub(r'[^A-Za-z\\s]', ' ', sent)\n",
    "  sent = re.sub(r'^\\s+|\\s+$', '', sent, flags=re.MULTILINE)\n",
    "  sent = re.sub(r'\\s{2,}', ' ', sent)\n",
    "  return sent.lower()\n",
    "\n",
    "\n",
    "def __tweet__preprocess(\n",
    "    tweets_df: pd.Series, method: Literal['stem', 'lemmatize']|None=None, misspelling=False, stopword=False\n",
    "    ) -> pd.Series:\n",
    "  stop_words = set(nltk_stopwords.words('english'))\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  stemmer    = PorterStemmer()\n",
    "  speller    = SpellChecker(distance=1)\n",
    "  speller.word_frequency.load_words(nltk_words.words())\n",
    "  def __preprocess_helper(tweet: str) -> str:\n",
    "    words = tweet.split()\n",
    "    clean_words: list[str] = []\n",
    "    for word in words:\n",
    "      if misspelling:\n",
    "        correct_word = speller.correction(word)\n",
    "        word = word if correct_word is None else correct_word\n",
    "      if stopword and word in stop_words:\n",
    "        continue\n",
    "      if method == 'stem':\n",
    "        word = stemmer.stem(word)\n",
    "      if method == 'lemmatize':\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "      clean_words.append(word)\n",
    "    return ' '.join(clean_words)\n",
    "  # begin preprocess\n",
    "  if method:\n",
    "     return tweets_df.apply(__preprocess_helper)\n",
    "  return tweets_df\n",
    "\n",
    "\n",
    "def __tweet__vectorizer(\n",
    "    tweets: pd.Series,\n",
    "    *,\n",
    "    w2v_model: object|None=None,\n",
    "    method: Literal['count', 'tfidf', 'word2vec'],\n",
    "    binary=False,\n",
    "    ngram: Literal['11', '12', '22']='11',\n",
    "  )-> np.ndarray:\n",
    "  ngram_range = {'11': (1, 1), '12': (1, 2), '22': (2, 2)}.get(ngram, (1, 1))\n",
    "  if method == 'tfidf':\n",
    "    bow = TfidfVectorizer(ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  elif method == 'count':\n",
    "    bow = CountVectorizer(binary=binary, ngram_range=ngram_range).fit_transform(tweets)\n",
    "    return bow # type: ignore\n",
    "  # block for Word2Vec\n",
    "  bow_w2v: np.ndarray = np.empty((0, 256))\n",
    "  for tweet in tweets:\n",
    "    tweet_vec = np.zeros((1, 256))\n",
    "    for word in tweet.split(' '):\n",
    "      tweet_vec += w2v_model[word]\n",
    "    bow_w2v = np.append(bow_w2v, tweet_vec, axis=0)\n",
    "  return bow_w2v\n",
    "\n",
    "\n",
    "def __load_tweets_and_labels_into_dataframe() -> pd.DataFrame:\n",
    "  # load dataset\n",
    "  neg, neu, pos = './data/negative.csv', './data/neutral.csv', './data/positive.csv'\n",
    "  with open(neg, 'r') as neg_f, open(neu, 'r') as neu_f, open(pos, 'r') as pos_f:\n",
    "    negative, neutral, positive = neg_f.read(), neu_f.read(), pos_f.read()\n",
    "  # sent tokenizing\n",
    "  sentence_pattern = r',([A-Z])'\n",
    "  replacement_pattern = r'\\n\\1'\n",
    "  negative = re.sub(sentence_pattern, replacement_pattern, negative).splitlines()\n",
    "  neutral  = re.sub(sentence_pattern, replacement_pattern, neutral ).splitlines()\n",
    "  positive = re.sub(sentence_pattern, replacement_pattern, positive).splitlines()\n",
    "  # create DataFrame for each label\n",
    "  negative = pd.DataFrame({'tweet': negative, 'label': 0})\n",
    "  neutral  = pd.DataFrame({'tweet': neutral, 'label': 1})\n",
    "  positive = pd.DataFrame({'tweet': positive, 'label': 2})\n",
    "  # concat DataFrame\n",
    "  tweets_df = pd.concat([negative, neutral, positive])\n",
    "  # remove empty tweets\n",
    "  tweets_df = tweets_df[tweets_df['tweet'].str.strip() != '']\n",
    "  tweets_df = tweets_df.drop_duplicates(subset=['tweet'], ignore_index=True)\n",
    "  tweets_df = tweets_df.sample(frac=1, ignore_index=True) # here shuffle tweets\n",
    "  return tweets_df\n",
    "\n",
    "\n",
    "def tweets_load() -> tuple[ArrayLike, ArrayLike, dict[str, np.ndarray]]:\n",
    "  \"\"\"\n",
    "  Return:\n",
    "  -------\n",
    "  tweets, y_labels, bows\n",
    "  \"\"\"\n",
    "  # processing and vectorizing with its params\n",
    "  processing  = ['just_tokenization', 'stemming', 'lemmatization', 'stemming+misspelling', 'lemmatization+misspelling', 'lemmatization+stopwords']\n",
    "  processing_params  = [\n",
    "    {'method': None}, {'method': 'stem'}, {'method': 'lemmatize'}, {'method': 'stem', 'misspelling': True},\n",
    "    {'method': 'lemmatize', 'misspelling': True}, {'method': 'lemmatize', 'stopword': True}\n",
    "    ]\n",
    "  vectorizing = ['binary', 'word_counts', 'tfidf', 'word2vec']\n",
    "  vectorizing_params = [{'method': 'count', 'binary': True}, {'method': 'count'}, {'method': 'tfidf'}, {'method': 'word2vec'}]\n",
    "  # other varibles\n",
    "  bows: dict[str, np.ndarray] = {}\n",
    "  # load datasets\n",
    "  tweets_df  = __load_tweets_and_labels_into_dataframe()\n",
    "\n",
    "  # -------------------- first: clean data -------------------------------------\n",
    "  tweets_df['tweet'] = tweets_df['tweet'].apply(__tweet__clean)\n",
    "  tweets_df = tweets_df[tweets_df['tweet'].str.strip() != '']\n",
    "  print('__tweet__clean data finished', end='\\r')\n",
    "\n",
    "  # ------------------ train Word2Vec model ------------------------------------\n",
    "  w2v_model = __train_fasttext__word2vec_model(tweets_df['tweet'])\n",
    "  print('word2vec model train finished', end='\\r')\n",
    "\n",
    "  # fit each processing method a vectorizer\n",
    "  __i, __len = 0, len(processing) * len(vectorizing) # -------------------------\n",
    "  for proc, proc_params in zip(processing, processing_params):\n",
    "    proc_tweets = __tweet__preprocess(tweets_df['tweet'], **proc_params)\n",
    "    for vect, vect_params in zip(vectorizing, vectorizing_params):\n",
    "      bows[f'{proc:27} and   {vect:13} vectorizing'] = __tweet__vectorizer(proc_tweets, w2v_model=w2v_model, **vect_params)\n",
    "      __i += 1\n",
    "      print(f'{__i:2}/{__len}:   {proc:27} and   {vect:13} vectorizing', end='\\r')\n",
    "  # return pure tweets, labels and bows\n",
    "  tweets, y = tweets_df['tweet'].values, tweets_df['label'].values\n",
    "  return tweets, y, bows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XdkXdYEEqb6"
   },
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "3p825hxREqb6"
   },
   "outputs": [],
   "source": [
    "# def get_top_10_most_similar_tweets(bow: np.ndarray, tweets: pd.DataFrame, tweet_idx: int) -> np.ndarray:\n",
    "#   bow_sum       = np.sqrt(np.sum(np.square(bow), axis=1))\n",
    "#   bow_dot_tweet = np.dot(bow, bow[tweet_idx])\n",
    "#   similarity    = np.divide(bow_dot_tweet, bow_sum * bow_sum[tweet_idx] + 1e-7)\n",
    "#   top_10_df     = pd.DataFrame({'0': similarity}).sort_values(by='0', ascending=False).drop(index=tweet_idx)[:10]\n",
    "#   top_10_index  = top_10_df[top_10_df['0'] > 0].index\n",
    "#   top_10_tweets = tweets['tweet'].loc[top_10_index].values\n",
    "#   return top_10_tweets # type: ignore\n",
    "\n",
    "\n",
    "def __top_similar_pairs(bow: np.ndarray) -> dict[tuple[int, int], float]:\n",
    "  similarity = pd.DataFrame(cosine_similarity(bow))\n",
    "  similar_pairs: dict[tuple[int, int], float] = {}\n",
    "  for tweet_idx in similarity.columns.values:\n",
    "    tweet_similiraty = similarity[tweet_idx].sort_values(ascending=False)\n",
    "    top_value, top_idx = tweet_similiraty[1], tweet_similiraty.index[1]\n",
    "    similar_pairs[(tweet_idx, top_idx)] = top_value\n",
    "  sorted_similar_pairs_by_value = {key: similar_pairs[key] for key in sorted(similar_pairs, key=lambda itm: similar_pairs[itm], reverse=True)}\n",
    "  return sorted_similar_pairs_by_value\n",
    "\n",
    "def __print_top_similar_tweets_pair(tweets: ArrayLike, bows: dict[str, np.ndarray]) -> dict[str, list]:\n",
    "  for bow_name in bows:\n",
    "    X = bows[bow_name]\n",
    "    top_similar_pairs = __top_similar_pairs(X)\n",
    "    print(\"top similar pairs of  '\", bow_name, \"':\", sep='')\n",
    "    __i = 1\n",
    "    for pair in top_similar_pairs:\n",
    "      first, second = pair\n",
    "      print(' ' * 15, '-' * 150)\n",
    "      print(' ' * 5, f'{1.0:3f}: {tweets[first]}')\n",
    "      print(' ' * 5, f'{top_similar_pairs[pair]:3f}: {tweets[second]}')\n",
    "      if __i == 10: break\n",
    "      __i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OirxTHbEqb7"
   },
   "source": [
    "### Just Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "liYBhEcjEqb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24:   lemmatization+stopwords     and   word2vec      vectorizing"
     ]
    }
   ],
   "source": [
    "tweets, y, bows = tweets_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Hf94w3mEqb-"
   },
   "source": [
    "### Global Varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "iBF16GHDEqb-"
   },
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "train_size: int = int(y.shape[0] * train_frac)\n",
    "\n",
    "y_train, y_test = y[: train_size], y[train_size: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNdAQJY7Eqb-"
   },
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "N26wpCrTEqb-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# __print_top_similar_tweets_pair(tweets, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Z1nKGr3sEqb-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72:   lemmatization+stopwords     and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 1.0}                               : 0.8262910798122066"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "bow_to_model: dict[str, Any] = {}\n",
    "\n",
    "__i, __len = 0, len(bows) * 3 # ( * number ) the number changes respectively with how many model you use in models: list\n",
    "for bow_name in bows:\n",
    "  X = bows[bow_name]\n",
    "  if isinstance(X, csr_matrix):\n",
    "    X = X.toarray()\n",
    "  X_train, X_test = X[: train_size], X[train_size: ]\n",
    "  models: list = [\n",
    "      LogisticRegression(max_iter=1337),\n",
    "      DecisionTreeClassifier(),\n",
    "      BernoulliNB(),\n",
    "    ]\n",
    "  param_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        'C': [0.1, 1.0],\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \"BernoulliNB\": {\n",
    "        'alpha': [0.1, 0.5, 1.0]\n",
    "    }\n",
    "  } \n",
    "  for model in models:\n",
    "\n",
    "    model_name = model.__class__.__name__\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=9)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_params_str = str(best_params)\n",
    "\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "    bow_to_model[f'{bow_name} using   {model_name:25} model with  {best_params_str:45}'] = accuracy\n",
    "    __i += 1\n",
    "    print(f'{__i:2}/{__len}:   {bow_name} using   {model_name:25} model with  {best_params_str:45}: {accuracy}', end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "zhm202TaEqb-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization+misspelling   and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9366197183098591\n",
      "lemmatization+misspelling   and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9342723004694836\n",
      "lemmatization+stopwords     and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9342723004694836\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9342723004694836\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9342723004694836\n",
      "lemmatization               and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.931924882629108\n",
      "lemmatization               and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.931924882629108\n",
      "stemming+misspelling        and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.931924882629108\n",
      "stemming                    and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9295774647887324\n",
      "stemming                    and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9295774647887324\n",
      "stemming+misspelling        and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9295774647887324\n",
      "just_tokenization           and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9272300469483568\n",
      "just_tokenization           and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9272300469483568\n",
      "just_tokenization           and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9248826291079812\n",
      "just_tokenization           and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9248826291079812\n",
      "stemming+misspelling        and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9248826291079812\n",
      "stemming+misspelling        and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9248826291079812\n",
      "stemming+misspelling        and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9248826291079812\n",
      "lemmatization+misspelling   and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 10}   :   0.9248826291079812\n",
      "just_tokenization           and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.9225352112676056\n",
      "just_tokenization           and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.9225352112676056\n",
      "just_tokenization           and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.9225352112676056\n",
      "lemmatization               and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9225352112676056\n",
      "lemmatization               and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9225352112676056\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9225352112676056\n",
      "stemming                    and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.92018779342723\n",
      "stemming                    and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.92018779342723\n",
      "stemming                    and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 5}    :   0.92018779342723\n",
      "stemming                    and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.92018779342723\n",
      "stemming                    and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.92018779342723\n",
      "lemmatization               and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.92018779342723\n",
      "lemmatization               and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.92018779342723\n",
      "lemmatization               and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.92018779342723\n",
      "stemming+misspelling        and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 5}    :   0.9178403755868545\n",
      "stemming+misspelling        and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9178403755868545\n",
      "stemming                    and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9154929577464789\n",
      "lemmatization               and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9154929577464789\n",
      "stemming+misspelling        and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9154929577464789\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9154929577464789\n",
      "lemmatization+stopwords     and   binary        vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9154929577464789\n",
      "lemmatization+stopwords     and   binary        vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 5}    :   0.9154929577464789\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 5}    :   0.9154929577464789\n",
      "just_tokenization           and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9131455399061033\n",
      "lemmatization               and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9131455399061033\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9131455399061033\n",
      "lemmatization+stopwords     and   word_counts   vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9131455399061033\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 2}    :   0.9131455399061033\n",
      "lemmatization+misspelling   and   binary        vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9107981220657277\n",
      "lemmatization+misspelling   and   word_counts   vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9107981220657277\n",
      "lemmatization+misspelling   and   tfidf         vectorizing using   BernoulliNB               model with  {'alpha': 0.5}                               :   0.9107981220657277\n",
      "just_tokenization           and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9084507042253521\n",
      "stemming                    and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 5}    :   0.9084507042253521\n",
      "stemming+misspelling        and   tfidf         vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 10}   :   0.9061032863849765\n",
      "just_tokenization           and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.9014084507042254\n",
      "lemmatization               and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8943661971830986\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8896713615023474\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8896713615023474\n",
      "lemmatization+stopwords     and   tfidf         vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8873239436619719\n",
      "stemming                    and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8779342723004695\n",
      "stemming+misspelling        and   word2vec      vectorizing using   LogisticRegression        model with  {'C': 1.0}                                   :   0.8755868544600939\n",
      "just_tokenization           and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 5, 'min_samples_split': 2}     :   0.8497652582159625\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 5, 'min_samples_split': 2}     :   0.8380281690140845\n",
      "stemming+misspelling        and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 5, 'min_samples_split': 2}     :   0.8333333333333334\n",
      "lemmatization               and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 5, 'min_samples_split': 10}    :   0.8286384976525821\n",
      "lemmatization+stopwords     and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 1.0}                               :   0.8262910798122066\n",
      "just_tokenization           and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.8215962441314554\n",
      "lemmatization               and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.8215962441314554\n",
      "stemming                    and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 5, 'min_samples_split': 2}     :   0.8192488262910798\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 1.0}                               :   0.8145539906103286\n",
      "stemming+misspelling        and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.8075117370892019\n",
      "stemming                    and   word2vec      vectorizing using   BernoulliNB               model with  {'alpha': 0.1}                               :   0.8051643192488263\n",
      "lemmatization+misspelling   and   word2vec      vectorizing using   DecisionTreeClassifier    model with  {'max_depth': 10, 'min_samples_split': 10}   :   0.7535211267605634\n"
     ]
    }
   ],
   "source": [
    "sorted_bow_to_model__keys = sorted(bow_to_model, key = lambda item: bow_to_model[item], reverse=True)\n",
    "\n",
    "for key in sorted_bow_to_model__keys:\n",
    "  value = bow_to_model[key]\n",
    "  print(f'{key}:   {value}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
